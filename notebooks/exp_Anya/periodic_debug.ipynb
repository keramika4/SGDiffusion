{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e297606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12e2430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeriodicEmbeddings(nn.Module):\n",
    "    def __init__(self, d_in, d_embedding, n_frequencies=16, max_frequency=10.0):\n",
    "        super().__init__()\n",
    "        self.d_in = d_in\n",
    "        self.n_frequencies = n_frequencies\n",
    "\n",
    "        freq_bands = torch.linspace(1.0, max_frequency, n_frequencies)\n",
    "        self.register_buffer('frequencies', freq_bands[None, :].repeat(d_in, 1))\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_in)\n",
    "        self.linear = nn.Linear(d_in * 2 * n_frequencies, d_embedding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        print(x.shape)\n",
    "        x_proj = 2 * math.pi * x.unsqueeze(-1) * self.frequencies\n",
    "        print(x_proj.shape)\n",
    "        x_pe = torch.cat([x_proj.sin(), x_proj.cos()], dim=-1)\n",
    "        print(x_pe.shape)\n",
    "        return self.linear(x_pe.view(x.size(0), -1))\n",
    "\n",
    "\n",
    "def make_mlp(in_dim, out_dim, hidden_dim, num_layers, activation='ReLU', dropout=0.0):\n",
    "    act_layer = getattr(nn, activation)\n",
    "    layers = [nn.Linear(in_dim, hidden_dim), act_layer(), nn.Dropout(dropout)]\n",
    "    for _ in range(num_layers):\n",
    "        layers += [nn.Linear(hidden_dim, hidden_dim), act_layer(), nn.Dropout(dropout)]\n",
    "    layers.append(nn.Linear(hidden_dim, out_dim))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class MLP_PLR(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size=28 * 28,\n",
    "        num_classes=10,\n",
    "        hidden_dim=128,\n",
    "        num_layers=2,\n",
    "        embedding_type='periodic',\n",
    "        d_embedding=128,\n",
    "        n_frequencies=32,\n",
    "        max_frequency=10.0,\n",
    "        activation='ReLU',\n",
    "        dropout=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if embedding_type == 'periodic':\n",
    "            self.embedding = PeriodicEmbeddings(\n",
    "                d_in=input_size,\n",
    "                d_embedding=d_embedding,\n",
    "                n_frequencies=n_frequencies,\n",
    "                max_frequency=max_frequency,\n",
    "            )\n",
    "            embedding_out_dim = d_embedding\n",
    "        elif embedding_type == 'none':\n",
    "            self.embedding = nn.Identity()\n",
    "            embedding_out_dim = input_size\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown embedding_type: {embedding_type}\")\n",
    "\n",
    "        self.network = make_mlp(\n",
    "            in_dim=embedding_out_dim,\n",
    "            out_dim=num_classes,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            activation=activation,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.embedding(x)\n",
    "        return self.network(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4da5c08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "periodic_emb = PeriodicEmbeddings(d_in=5, d_embedding=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06ad9aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "198c6ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n",
      "torch.Size([1, 5, 16])\n",
      "torch.Size([1, 5, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = periodic_emb(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56582cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import statistics\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Literal, cast\n",
    "\n",
    "\n",
    "def cos_sin(x: Tensor) -> Tensor:\n",
    "    return torch.cat([torch.cos(x), torch.sin(x)], -1)\n",
    "\n",
    "class PeriodicOptions:\n",
    "    n: int  # the output size is 2 * n\n",
    "    sigma: float\n",
    "    trainable: bool\n",
    "    initialization: Literal['log-linear', 'normal']\n",
    "\n",
    "\n",
    "class Periodic(nn.Module):\n",
    "    def __init__(self, n_features: int, options: PeriodicOptions) -> None:\n",
    "        super().__init__()\n",
    "        if options.initialization == 'log-linear':\n",
    "            coefficients = options.sigma ** (torch.arange(options.n) / options.n)\n",
    "            coefficients = coefficients[None].repeat(n_features, 1)\n",
    "        else:\n",
    "            assert options.initialization == 'normal'\n",
    "            coefficients = torch.normal(0.0, options.sigma, (n_features, options.n))\n",
    "        if options.trainable:\n",
    "            self.coefficients = nn.Parameter(coefficients)  # type: ignore[code]\n",
    "        else:\n",
    "            self.register_buffer('coefficients', coefficients)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        assert x.ndim == 2\n",
    "        return cos_sin(2 * torch.pi * self.coefficients[None] * x[..., None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdb1c3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = PeriodicOptions()\n",
    "options.n = 10\n",
    "options.sigma = 0.3\n",
    "options.trainable = True\n",
    "options.initialization = 'normal' \n",
    "periodic_emb = Periodic(n_features = 5 , options= options )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4d34206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 20])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = periodic_emb(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d38d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "\n",
    "def _check_input_shape(x: Tensor, expected_n_features: int) -> None:\n",
    "    if x.ndim < 1:\n",
    "        raise ValueError(\n",
    "            f'The input must have at least one dimension, however: {x.ndim=}'\n",
    "        )\n",
    "    if x.shape[-1] != expected_n_features:\n",
    "        raise ValueError(\n",
    "            'The last dimension of the input was expected to be'\n",
    "            f' {expected_n_features}, however, {x.shape[-1]=}'\n",
    "        )\n",
    "class _Periodic(nn.Module):\n",
    "    \"\"\"\n",
    "    NOTE: THIS MODULE SHOULD NOT BE USED DIRECTLY.\n",
    "\n",
    "    Technically, this is a linear embedding without bias followed by\n",
    "    the periodic activations. The scale of the initialization\n",
    "    (defined by the `sigma` argument) plays an important role.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_features: int, k: int, sigma: float) -> None:\n",
    "        if sigma <= 0.0:\n",
    "            raise ValueError(f'sigma must be positive, however: {sigma=}')\n",
    "\n",
    "        super().__init__()\n",
    "        self._sigma = sigma\n",
    "        self.weight = Parameter(torch.empty(n_features, k))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reset the parameters.\"\"\"\n",
    "        # NOTE[DIFF]\n",
    "        # Here, extreme values (~0.3% probability) are explicitly avoided just in case.\n",
    "        # In the paper, there was no protection from extreme values.\n",
    "        bound = self._sigma * 3\n",
    "        nn.init.trunc_normal_(self.weight, 0.0, self._sigma, a=-bound, b=bound)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Do the forward pass.\"\"\"\n",
    "        _check_input_shape(x, self.weight.shape[0])\n",
    "        x = 2 * math.pi * self.weight * x[..., None]\n",
    "        x = torch.cat([torch.cos(x), torch.sin(x)], -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "# _NLinear is a simplified copy of delu.nn.NLinear:\n",
    "# https://yura52.github.io/delu/stable/api/generated/delu.nn.NLinear.html\n",
    "class _NLinear(nn.Module):\n",
    "    \"\"\"N *separate* linear layers for N feature embeddings.\n",
    "\n",
    "    In other words,\n",
    "    each feature embedding is transformed by its own dedicated linear layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, n: int, in_features: int, out_features: int, bias: bool = True\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.weight = Parameter(torch.empty(n, in_features, out_features))\n",
    "        self.bias = Parameter(torch.empty(n, out_features)) if bias else None\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reset the parameters.\"\"\"\n",
    "        d_in_rsqrt = self.weight.shape[-2] ** -0.5\n",
    "        nn.init.uniform_(self.weight, -d_in_rsqrt, d_in_rsqrt)\n",
    "        if self.bias is not None:\n",
    "            nn.init.uniform_(self.bias, -d_in_rsqrt, d_in_rsqrt)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Do the forward pass.\"\"\"\n",
    "        if x.ndim != 3:\n",
    "            raise ValueError(\n",
    "                '_NLinear supports only inputs with exactly one batch dimension,'\n",
    "                ' so `x` must have a shape like (BATCH_SIZE, N_FEATURES, D_EMBEDDING).'\n",
    "            )\n",
    "        assert x.shape[-(self.weight.ndim - 1) :] == self.weight.shape[:-1]\n",
    "\n",
    "        x = x.transpose(0, 1)\n",
    "        x = x @ self.weight\n",
    "        x = x.transpose(0, 1)\n",
    "        if self.bias is not None:\n",
    "            x = x + self.bias\n",
    "        return x\n",
    "\n",
    "\n",
    "class PeriodicEmbeddings(nn.Module):\n",
    "    \"\"\"Embeddings for continuous features based on periodic activations.\n",
    "\n",
    "    See README for details.\n",
    "\n",
    "    **Shape**\n",
    "\n",
    "    - Input: `(*, n_features)`\n",
    "    - Output: `(*, n_features, d_embedding)`\n",
    "\n",
    "    **Examples**\n",
    "\n",
    "    >>> batch_size = 2\n",
    "    >>> n_cont_features = 3\n",
    "    >>> x = torch.randn(batch_size, n_cont_features)\n",
    "    >>>\n",
    "    >>> d_embedding = 24\n",
    "    >>> m = PeriodicEmbeddings(n_cont_features, d_embedding, lite=False)\n",
    "    >>> m(x).shape\n",
    "    torch.Size([2, 3, 24])\n",
    "    >>>\n",
    "    >>> m = PeriodicEmbeddings(n_cont_features, d_embedding, lite=True)\n",
    "    >>> m(x).shape\n",
    "    torch.Size([2, 3, 24])\n",
    "    >>>\n",
    "    >>> # PL embeddings.\n",
    "    >>> m = PeriodicEmbeddings(n_cont_features, d_embedding=8, activation=False, lite=False)\n",
    "    >>> m(x).shape\n",
    "    torch.Size([2, 3, 8])\n",
    "    \"\"\"  # noqa: E501\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features: int,\n",
    "        d_embedding: int = 24,\n",
    "        *,\n",
    "        n_frequencies: int = 48,\n",
    "        frequency_init_scale: float = 0.01,\n",
    "        activation: bool = True,\n",
    "        lite: bool,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_features: the number of features.\n",
    "            d_embedding: the embedding size.\n",
    "            n_frequencies: the number of frequencies for each feature.\n",
    "                (denoted as \"k\" in Section 3.3 in the paper).\n",
    "            frequency_init_scale: the initialization scale for the first linear layer\n",
    "                (denoted as \"sigma\" in Section 3.3 in the paper).\n",
    "                **This is an important hyperparameter**, see README for details.\n",
    "            activation: if `False`, the ReLU activation is not applied.\n",
    "                Must be `True` if ``lite=True``.\n",
    "            lite: if True, the outer linear layer is shared between all features.\n",
    "                See README for details.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.periodic = _Periodic(n_features, n_frequencies, frequency_init_scale)\n",
    "        self.linear: Union[nn.Linear, _NLinear]\n",
    "        if lite:\n",
    "            # NOTE[DIFF]\n",
    "            # The lite variation was introduced in a different paper\n",
    "            # (about the TabR model).\n",
    "            if not activation:\n",
    "                raise ValueError('lite=True is allowed only when activation=True')\n",
    "            self.linear = nn.Linear(2 * n_frequencies, d_embedding)\n",
    "        else:\n",
    "            self.linear = _NLinear(n_features, 2 * n_frequencies, d_embedding)\n",
    "        self.activation = nn.ReLU() if activation else None\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Do the forward pass.\"\"\"\n",
    "        x = self.periodic(x)\n",
    "        x = self.linear(x)\n",
    "        if self.activation is not None:\n",
    "            x = self.activation(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
